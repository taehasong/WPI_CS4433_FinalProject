[0m[[0m[31merror[0m] [0m[0morg.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 44.0 failed 1 times, most recent failure: Lost task 5.0 in stage 44.0 (TID 361) (Taeha.lan executor driver): java.lang.OutOfMemoryError: Java heap space[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$10991/0x00000001021a8c28.apply(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Array$.tabulate(Array.scala:418)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$10834/0x0000000102158c28.apply(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$Lambda$8214/0x000000010212b028.apply(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.Task.run(Task.scala:141)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$7964/0x0000000101e7f428.apply(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:750)[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mDriver stacktrace:[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.foreach(Option.scala:407)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.collect(RDD.scala:1045)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.Predictor.fit(Predictor.scala:78)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator.foreach(Iterator.scala:943)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator.foreach$(Iterator.scala:943)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)[0m
[0m[[0m[31merror[0m] [0m[0m	at BonusPoint$.$anonfun$main$1(BonusPoint.scala:99)[0m
[0m[[0m[31merror[0m] [0m[0m	at BonusPoint$.$anonfun$main$1$adapted(BonusPoint.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.immutable.List.foreach(List.scala:431)[0m
[0m[[0m[31merror[0m] [0m[0m	at BonusPoint$.main(BonusPoint.scala:97)[0m
[0m[[0m[31merror[0m] [0m[0m	at BonusPoint.main(BonusPoint.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Method.java:498)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:135)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:85)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:112)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:178)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:112)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:2072)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:2011)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:378)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:750)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.lang.OutOfMemoryError: Java heap space[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$10991/0x00000001021a8c28.apply(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Array$.tabulate(Array.scala:418)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$10834/0x0000000102158c28.apply(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$Lambda$8214/0x000000010212b028.apply(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.Task.run(Task.scala:141)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$7964/0x0000000101e7f428.apply(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:750)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 44.0 failed 1 times, most recent failure: Lost task 5.0 in stage 44.0 (TID 361) (Taeha.lan executor driver): java.lang.OutOfMemoryError: Java heap space[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$10991/0x00000001021a8c28.apply(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Array$.tabulate(Array.scala:418)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$10834/0x0000000102158c28.apply(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$Lambda$8214/0x000000010212b028.apply(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.Task.run(Task.scala:141)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner$$Lambda$7964/0x0000000101e7f428.apply(Unknown Source)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:750)[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mDriver stacktrace:[0m
